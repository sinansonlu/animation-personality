{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Animation Samples and Preparing ZeroEGGS Subsamples"
      ],
      "metadata": {
        "id": "fyE0BG9ZuYed"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcFbZM-wFBSt"
      },
      "outputs": [],
      "source": [
        "# download zeroeggs zip files: https://github.com/ubisoft/ubisoft-laforge-ZeroEGGS.git\n",
        "!wget https://github.com/ubisoft/ubisoft-laforge-ZeroEGGS/raw/main/data/Zeggs_data.zip\n",
        "!wget https://github.com/ubisoft/ubisoft-laforge-ZeroEGGS/raw/main/data/Zeggs_data.z01\n",
        "!wget https://github.com/ubisoft/ubisoft-laforge-ZeroEGGS/raw/main/data/Zeggs_data.z02\n",
        "\n",
        "# extract split zip files\n",
        "!7z x Zeggs_data.zip\n",
        "\n",
        "# download subsample times: https://github.com/sinansonlu/animation-personality.git\n",
        "!wget https://github.com/sinansonlu/animation-personality/raw/main/zeroeggs_subsample_times.zip\n",
        "\n",
        "# extract subsample times zip file\n",
        "!7z x zeroeggs_subsample_times.zip"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install blender for cutting zeroeggs samples\n",
        "!pip install bpy"
      ],
      "metadata": {
        "id": "cLoqHh8YKGKR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import bpy\n",
        "import os\n",
        "from glob import glob\n",
        "from natsort import natsorted\n",
        "\n",
        "# directory for zeroeggs bvh files\n",
        "directory = 'clean/'\n",
        "\n",
        "# text files for subsample times\n",
        "txt_files = natsorted(glob(os.path.join('', '*.txt')))\n",
        "\n",
        "# save directory for subsamples\n",
        "savepath = 'zeroeggs_subsamples/'\n",
        "!mkdir zeroeggs_subsamples\n",
        "\n",
        "partcount = 0 # indicates the index of subsample\n",
        "mainbvhindex = 1 # indicates the original sample where this subsample comes from\n",
        "\n",
        "for txtfile in txt_files:\n",
        "    # remove any object in scene\n",
        "    bpy.ops.object.select_all(action='SELECT')\n",
        "    bpy.ops.object.delete()\n",
        "\n",
        "    # bvh file name ends with bvh instead of txt\n",
        "    bvhfile = txtfile.replace('txt','bvh')\n",
        "\n",
        "    # import bvh animation\n",
        "    bpy.ops.import_anim.bvh(filepath=directory+bvhfile, filter_glob='*.bvh', target='ARMATURE', global_scale=1.0, frame_start=1, use_fps_scale=False, update_scene_fps=False, update_scene_duration=False, use_cyclic=False, rotate_mode='NATIVE', axis_forward='-Z', axis_up='Y')\n",
        "\n",
        "    scene = bpy.context.scene\n",
        "\n",
        "    # part indexes for this sample will start from 0\n",
        "    partind = 0\n",
        "\n",
        "    # make subsample bvh for each marked part\n",
        "    with open(txtfile, \"r\") as filestream:\n",
        "        for line in filestream:\n",
        "            currentline = line.split(',')\n",
        "\n",
        "            # mark the start and end of the subsample\n",
        "            start = float(currentline[0])\n",
        "            end = float(currentline[1])\n",
        "\n",
        "            # convert to frame number, ZeroEGGS is 60 FPS\n",
        "            scene.frame_start = int(start * 60)\n",
        "            scene.frame_end = int(end * 60)\n",
        "\n",
        "            # export bvh\n",
        "            bpy.ops.export_anim.bvh(filepath=savepath + 'n_' + str(partcount) + '_o_' + str(mainbvhindex) + '_' + str(partind) + '.bvh',\n",
        "                check_existing=True,\n",
        "                filter_glob='*.bvh',\n",
        "                global_scale=1.0,\n",
        "                rotate_mode='NATIVE',\n",
        "                root_transform_only=False)\n",
        "\n",
        "            partcount += 1\n",
        "            partind += 1\n",
        "\n",
        "    mainbvhindex += 1"
      ],
      "metadata": {
        "id": "B69NtrzWGagX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zip and download subsamples for future use\n",
        "!zip -r zeroeggs_subsamples.zip zeroeggs_subsamples\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"zeroeggs_subsamples.zip\")"
      ],
      "metadata": {
        "id": "8uneOsLzx9-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# continue point for unzipping zipped samples, ignore if first time running\n",
        "!unzip zeroeggs_subsamples.zip"
      ],
      "metadata": {
        "id": "vVUmPkHW4WU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Bandai repository\n",
        "!git clone https://github.com/BandaiNamcoResearchInc/Bandai-Namco-Research-Motiondataset.git"
      ],
      "metadata": {
        "id": "vJ-c0Ugtv5ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy Bandai data to root for ease of access\n",
        "!mkdir bandai_samples\n",
        "!cp -r Bandai-Namco-Research-Motiondataset/dataset/Bandai-Namco-Research-Motiondataset-1/data/* bandai_samples"
      ],
      "metadata": {
        "id": "IrlJ0aVQ1UNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eliminating ZeroEGGS Samples"
      ],
      "metadata": {
        "id": "TdDQ2sjL1M2l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install bvhio\n",
        "!pip install bvhio"
      ],
      "metadata": {
        "id": "J2Nkey511SEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test bvhio\n",
        "import bvhio\n",
        "\n",
        "path = 'zeroeggs_subsamples/n_0_o_1_0.bvh'\n",
        "\n",
        "root = bvhio.readAsHierarchy(path)\n",
        "root.printTree()\n",
        "\n",
        "root.loadRestPose(recursive=True)\n",
        "print('\\nRest pose position and Y-direction of each joint in world space ')\n",
        "for joint, index, depth in root.layout():\n",
        "    print(f'{joint.PositionWorld} {joint.UpWorld} {joint.Name}')"
      ],
      "metadata": {
        "id": "VgWMHM-j2opS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate features from ZeroEGGS subsample BVHs and generate feature vectors as pickles\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "import bvhio\n",
        "import glm\n",
        "import numpy as np\n",
        "\n",
        "feature_size = 21\n",
        "\n",
        "min_element_size = 1\n",
        "max_element_size = 16\n",
        "\n",
        "directory = 'zeroeggs_subsamples'\n",
        "\n",
        "bodyParts = ['Spine', 'LeftHand', 'RightHand', 'Head', 'RightFoot','LeftFoot', 'RightArm',\n",
        "             'LeftArm', 'Hips', 'RightForeArm','LeftForeArm', 'Neck', 'LeftLeg','RightLeg',\n",
        "             'LeftUpLeg', 'RightUpLeg']\n",
        "\n",
        "def partNameToIndex(partName):\n",
        "    return bodyParts.index(partName)\n",
        "\n",
        "def makeVectors(animindex):\n",
        "    vecs_all = []\n",
        "    for i in range(min_element_size,max_element_size):\n",
        "        vecs_all.append(makeVectorOfSize(animindex,i))\n",
        "    return vecs_all\n",
        "\n",
        "def makeVectorOfSize(animindex,size):\n",
        "    arr = np.zeros((feature_size,size))\n",
        "    joints = jointsall[animindex]\n",
        "    frame_count = len(joints)\n",
        "\n",
        "    num_of_frames_per_element = int(frame_count / size)\n",
        "\n",
        "    for i in range(0,size):\n",
        "        arr[0,i] = calcAvgDistance(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'LeftHand','RightHand')\n",
        "        arr[1,i] = calcAvgDistance(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'Head','RightHand')\n",
        "        arr[2,i] = calcAvgDistance(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'Head','LeftHand')\n",
        "        arr[3,i] = calcAvgDistance(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'RightFoot','LeftFoot')\n",
        "        arr[4,i] = calcAvgDistance(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'RightArm','LeftArm')\n",
        "        arr[5,i] = calcAvgDistance(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'Hips','Head')\n",
        "        arr[6,i] = calcAvgDistance(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'LeftLeg','RightLeg')\n",
        "        arr[7,i] = calcAvgDistance(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'RightForeArm','LeftForeArm')\n",
        "        arr[8,i] = calcAvgDistance(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'Hips','RightHand')\n",
        "        arr[9,i] = calcAvgDistance(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'Hips','LeftHand')\n",
        "\n",
        "        arr[10,i] = calcAvgSpeed(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'LeftHand')\n",
        "        arr[11,i] = calcAvgSpeed(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'RightHand')\n",
        "        arr[12,i] = calcAvgSpeed(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'LeftForeArm')\n",
        "        arr[13,i] = calcAvgSpeed(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'RightForeArm')\n",
        "        arr[14,i] = calcAvgSpeed(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'Head')\n",
        "        arr[15,i] = calcAvgSpeed(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'Neck')\n",
        "\n",
        "        arr[16,i] = calcAvgAngle(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'LeftHand','LeftForeArm','LeftArm')\n",
        "        arr[17,i] = calcAvgAngle(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'RightHand','RightForeArm','RightArm')\n",
        "        arr[18,i] = calcAvgAngle(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'Spine','Neck','Head')\n",
        "        arr[19,i] = calcAvgAngle(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'LeftUpLeg','LeftLeg','LeftFoot')\n",
        "        arr[20,i] = calcAvgAngle(joints,i*num_of_frames_per_element,(i+1)*num_of_frames_per_element,'RightUpLeg','RightLeg','RightFoot')\n",
        "\n",
        "    return arr\n",
        "\n",
        "def calcAvgSpeed(joints,pose_no_begin,pose_no_end,joint):\n",
        "    total = 0\n",
        "    count = 0\n",
        "    for i in range(pose_no_begin,pose_no_end-1):\n",
        "        total += speed(joints,i,i+1,joint)\n",
        "        count += 1\n",
        "    if count > 0:\n",
        "        return total / count\n",
        "    else:\n",
        "        return total\n",
        "\n",
        "def calcAvgDistance(joints,pose_no_begin,pose_no_end,joint_a,joint_b):\n",
        "    total = 0\n",
        "    count = 0\n",
        "    for i in range(pose_no_begin,pose_no_end):\n",
        "        total += distance(joints,i,joint_a,joint_b)\n",
        "        count += 1\n",
        "    if count > 0:\n",
        "        return total / count\n",
        "    else:\n",
        "        return total\n",
        "\n",
        "def calcAvgAngle(joints,pose_no_begin,pose_no_end,joint_a,joint_cent,joint_b):\n",
        "    total = 0\n",
        "    count = 0\n",
        "    for i in range(pose_no_begin,pose_no_end):\n",
        "        total += angle(joints,i,joint_a,joint_cent,joint_b)\n",
        "        count += 1\n",
        "    if count > 0:\n",
        "        return total / count\n",
        "    else:\n",
        "        return total\n",
        "\n",
        "def distance(joints,pose_no,joint_a,joint_b):\n",
        "    return glm.distance(joints[pose_no][partNameToIndex(joint_a)],joints[pose_no][partNameToIndex(joint_b)])\n",
        "\n",
        "def speed(joints,pose_no_start,pose_no_end,joint_a):\n",
        "    return glm.distance(joints[pose_no_start][partNameToIndex(joint_a)],joints[pose_no_end][partNameToIndex(joint_a)])\n",
        "\n",
        "def angle(joints,pose_no,joint_a,joint_cent,joint_b):\n",
        "    posCent = joints[pose_no][partNameToIndex(joint_cent)]\n",
        "    v1 = posCent - joints[pose_no][partNameToIndex(joint_a)]\n",
        "    v2 = posCent - joints[pose_no][partNameToIndex(joint_b)]\n",
        "    return glm.acos(glm.dot(glm.normalize(v1),glm.normalize(v2)))\n",
        "\n",
        "# get joint word points to array\n",
        "joint_per_pose = len(bodyParts)\n",
        "\n",
        "jointsall = []\n",
        "counti = 0\n",
        "\n",
        "!mkdir Pickles\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "    if os.path.isfile(f):\n",
        "        print(str(counti) + ' - Processing ' + filename)\n",
        "        start_time = time.time()\n",
        "\n",
        "        root = bvhio.readAsHierarchy(f)\n",
        "        frame_count = len(root.Keyframes)\n",
        "        joints = np.empty((frame_count,joint_per_pose),dtype=object)\n",
        "        for frame in range(frame_count):\n",
        "          loadedPose = root.loadPose(frame)\n",
        "          for partin,part in enumerate(bodyParts):\n",
        "            joints[frame][partin] = loadedPose.filter(part)[0].PositionWorld\n",
        "        jointsall.append(joints)\n",
        "\n",
        "        vecs = makeVectors(counti)\n",
        "        file = open('Pickles/' + filename.split('.')[0] + '.pickle', 'wb')\n",
        "        pickle.dump(vecs, file)\n",
        "        file.close()\n",
        "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "        counti += 1"
      ],
      "metadata": {
        "id": "uI7Ps6Ig2xEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read from pickles\n",
        "import pickle\n",
        "\n",
        "pickleDict = {}\n",
        "\n",
        "directory = 'Pickles'\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    f = os.path.join(directory, filename)\n",
        "    if os.path.isfile(f):\n",
        "        file = open(f, 'rb')\n",
        "        pickleDict[filename.split('.')[0]] = pickle.load(file)\n",
        "        file.close()\n",
        "\n",
        "count = 0\n",
        "for p in pickleDict:\n",
        "    count += 1\n",
        "print(\"Read file count \" + str(count))\n",
        "\n",
        "# normalize per element size and feature\n",
        "import numpy as np\n",
        "\n",
        "def getMinVal(fname,element_size,feat):\n",
        "    vec_of_file = pickleDict[fname]\n",
        "    vec_of_size = vec_of_file[element_size-1]\n",
        "    vals = vec_of_size[feat]\n",
        "    minval = vals[0]\n",
        "    for i in range(element_size):\n",
        "        if minval > vals[i]:\n",
        "            minval = vals[i]\n",
        "    return minval\n",
        "\n",
        "def getMaxVal(fname,element_size,feat):\n",
        "    vec_of_file = pickleDict[fname]\n",
        "    vec_of_size = vec_of_file[element_size-1]\n",
        "    vals = vec_of_size[feat]\n",
        "    maxval = vals[0]\n",
        "    for i in range(element_size):\n",
        "        if maxval < vals[i]:\n",
        "            maxval = vals[i]\n",
        "    return maxval\n",
        "\n",
        "def findMaxOf(element_size,feat):\n",
        "    maxval = getMaxVal(next(iter(pickleDict.keys())),element_size,feat)\n",
        "    for p in pickleDict:\n",
        "        maxc = getMaxVal(p,element_size,feat)\n",
        "        if maxc > maxval:\n",
        "            maxval = maxc\n",
        "    return maxval\n",
        "\n",
        "def findMinOf(element_size,feat):\n",
        "    minval = getMinVal(next(iter(pickleDict.keys())),element_size,feat)\n",
        "    for p in pickleDict:\n",
        "        minc = getMinVal(p,element_size,feat)\n",
        "        if minc < minval:\n",
        "            minval = minc\n",
        "    return minval\n",
        "\n",
        "def normalizeVec(max_value,min_value,fname,element_size,feat):\n",
        "    vec_of_file = pickleDict[fname]\n",
        "    vec_of_size = vec_of_file[element_size-1]\n",
        "    vals = vec_of_size[feat]\n",
        "    for i in range(element_size):\n",
        "        vals[i] = (vals[i] - min_value) / (max_value - min_value)\n",
        "\n",
        "def normalize(max_value,min_value,element_size,feat):\n",
        "    for p in pickleDict:\n",
        "        normalizeVec(max_value,min_value,p,element_size,feat)\n",
        "\n",
        "max_per_feature_and_size = np.zeros((max_element_size-min_element_size,feature_size))\n",
        "min_per_feature_and_size = np.zeros((max_element_size-min_element_size,feature_size))\n",
        "\n",
        "for element_size in range(min_element_size,max_element_size):\n",
        "    for feat in range(feature_size):\n",
        "        min_per_feature_and_size[element_size-min_element_size,feat] = findMinOf(element_size,feat)\n",
        "        max_per_feature_and_size[element_size-min_element_size,feat] = findMaxOf(element_size,feat)\n",
        "        normalize(max_per_feature_and_size[element_size-min_element_size,feat], min_per_feature_and_size[element_size-min_element_size,feat],element_size,feat)\n",
        "\n",
        "# validate ranges\n",
        "mins = 0\n",
        "maxs = 0\n",
        "count = 0\n",
        "for element_size in range(min_element_size,max_element_size):\n",
        "    for feat in range(feature_size):\n",
        "        mins += findMinOf(element_size,feat)\n",
        "        maxs += findMaxOf(element_size,feat)\n",
        "        count += 1\n",
        "print(mins/count)\n",
        "print(maxs/count)"
      ],
      "metadata": {
        "id": "8w08E5vG6w1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make distances matrix for pairwise similarity of subsamples\n",
        "def distanceBtwFeat(fname1,fname2,element_size,feat):\n",
        "    vec_of_file1 = pickleDict[fname1]\n",
        "    vec_of_size1 = vec_of_file1[element_size-1]\n",
        "    vals1 = vec_of_size1[feat]\n",
        "\n",
        "    vec_of_file2 = pickleDict[fname2]\n",
        "    vec_of_size2 = vec_of_file2[element_size-1]\n",
        "    vals2 = vec_of_size2[feat]\n",
        "\n",
        "    distance = 0\n",
        "    count = 0\n",
        "    for i in range(element_size):\n",
        "        distance += abs(vals1[i] - vals2[i])\n",
        "        count += 1\n",
        "    return (distance / count)\n",
        "\n",
        "def distanceBtw(fname1,fname2):\n",
        "    totalDistance = 0\n",
        "    for element_size in range(min_element_size,max_element_size):\n",
        "        for feat in range(feature_size):\n",
        "            totalDistance += distanceBtwFeat(fname1,fname2,element_size,feat)\n",
        "    return totalDistance\n",
        "\n",
        "def indexFromFname(fname):\n",
        "    return int(fname.split('_')[1])\n",
        "\n",
        "distanceMatrix = np.zeros((len(pickleDict),len(pickleDict)))\n",
        "fcount = 0\n",
        "\n",
        "for p1 in pickleDict:\n",
        "    fcount += 1\n",
        "    print('calculating for ' + p1 + ' (' + str(fcount) + '/1291)', end='\\r')\n",
        "\n",
        "    for p2 in pickleDict:\n",
        "        if p1 != p2:\n",
        "            distanceMatrix[indexFromFname(p1),indexFromFname(p2)] = distanceBtw(p1,p2)\n",
        "        else:\n",
        "            distanceMatrix[indexFromFname(p1),indexFromFname(p2)] = float('inf')\n",
        "\n",
        "# save distance matrix\n",
        "import pickle\n",
        "file = open('distanceMatrix.pickle', 'wb')\n",
        "pickle.dump(distanceMatrix, file)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "-sS_6Ioo68fy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load distance matrix for eliminating the similar samples\n",
        "file = open('distanceMatrix.pickle', 'rb')\n",
        "distM = pickle.load(file)\n",
        "file.close()\n",
        "\n",
        "totalRemaining = len(pickleDict)\n",
        "targetCount = 100 # we will reduce to 100 samples\n",
        "eliminateds = []\n",
        "\n",
        "def find_min_idx(x):\n",
        "    k = x.argmin()\n",
        "    ncol = x.shape[1]\n",
        "    return int(k/ncol), k%ncol\n",
        "\n",
        "while(totalRemaining > targetCount):\n",
        "    # find the closest samples\n",
        "    i,j = find_min_idx(distM)\n",
        "\n",
        "    # find the one that is closer to others, exluding infinites\n",
        "    sumi = 0\n",
        "    sumj = 0\n",
        "    for n in range(len(pickleDict)):\n",
        "        if(distM[i,n] != float('inf')):\n",
        "            sumi += distM[i,n]\n",
        "        if(distM[j,n] != float('inf')):\n",
        "            sumj += distM[j,n]\n",
        "\n",
        "    to_eliminate = i\n",
        "\n",
        "    if sumi > sumj:\n",
        "        to_eliminate = j\n",
        "\n",
        "    # eliminate by setting to infinity\n",
        "    distM[to_eliminate,:] = float('inf')\n",
        "    distM[:,to_eliminate] = float('inf')\n",
        "    eliminateds.append(to_eliminate)\n",
        "    totalRemaining -=1\n",
        "    print('Eliminated ' + str(to_eliminate) + ' in comparision btw ' + str(i) + ' - ' + str(j))\n",
        "\n",
        "# get the remaining ones\n",
        "remains = []\n",
        "for i in range(len(pickleDict)):\n",
        "    if i not in eliminateds:\n",
        "        remains.append(i)"
      ],
      "metadata": {
        "id": "7x3CjaXZ7ZHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy remaining ones to different folder\n",
        "import shutil\n",
        "\n",
        "!mkdir zeroeggs_remaining_subsamples\n",
        "\n",
        "def getFnameFromIndex(index):\n",
        "    search_key = 'n_' + str(index) + '_'\n",
        "    res = [key for key, val in pickleDict.items() if search_key in key]\n",
        "    return res[0]\n",
        "\n",
        "for r in remains:\n",
        "    shutil.copy2('zeroeggs_subsamples/' + getFnameFromIndex(r) + '.bvh', 'zeroeggs_remaining_subsamples/')"
      ],
      "metadata": {
        "id": "KPNBCa1g7v3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zip and download remaining subsamples for future use\n",
        "!zip -r zeroeggs_remaining_subsamples.zip zeroeggs_remaining_subsamples\n",
        "\n",
        "from google.colab import files\n",
        "files.download(\"zeroeggs_remaining_subsamples.zip\")"
      ],
      "metadata": {
        "id": "_uIEvwqHUvpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Pose Data For Animation Analysis"
      ],
      "metadata": {
        "id": "tw0rAsrKvIOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checkout repository to access ZeroEGGS subsamples\n",
        "!git clone https://github.com/sinansonlu/animation-personality.git"
      ],
      "metadata": {
        "id": "zkXKeprktEvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# install bvhio\n",
        "!pip install bvhio"
      ],
      "metadata": {
        "id": "yDm7iLwKwiyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare numpy arrays using joint world positions of the remaining ZeroEGGS subsamples\n",
        "import os\n",
        "import pickle\n",
        "import bvhio\n",
        "import numpy as np\n",
        "from natsort import natsorted\n",
        "\n",
        "bodyParts_zeroeggs = ['Hips', 'Spine', 'Spine2', 'Neck', 'Head', 'LeftShoulder', 'LeftArm', 'LeftForeArm', 'LeftHand',\n",
        "            'RightShoulder', 'RightArm', 'RightForeArm', 'RightHand', 'LeftUpLeg', 'LeftLeg', 'LeftFoot',\n",
        "            'LeftToeBase', 'RightUpLeg', 'RightLeg', 'RightFoot', 'RightToeBase']\n",
        "\n",
        "joint_per_pose = len(bodyParts_zeroeggs)\n",
        "\n",
        "directory_zeroeggs = \"animation-personality/zeroeggs_remaining_subsamples\"\n",
        "bvhanis_zeroeggs = []\n",
        "\n",
        "for filename in natsorted(os.listdir(directory_zeroeggs)):\n",
        "    f = os.path.join(directory, filename)\n",
        "    if os.path.isfile(f):\n",
        "        bvhanis_zeroeggs.append(bvhio.readAsHierarchy(f))\n",
        "\n",
        "jointsall_zeroeggs = []\n",
        "\n",
        "for ind, root in enumerate(bvhanis_zeroeggs):\n",
        "    frame_count = len(root.Keyframes)\n",
        "    joints = np.empty((frame_count,joint_per_pose),dtype=object)\n",
        "\n",
        "    for frame in range(frame_count):\n",
        "        loadedPose = root.loadPose(frame)\n",
        "\n",
        "        for partin,part in enumerate(bodyParts_zeroeggs):\n",
        "            joints[frame][partin] = loadedPose.filter(part)[0].PositionWorld\n",
        "\n",
        "    jointsall_zeroeggs.append(joints)\n",
        "\n",
        "file = open('zeroeggs_joints.pickle', 'wb')\n",
        "pickle.dump(jointsall_zeroeggs, file)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "jgmFWgBlyIwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clone Bandai repository\n",
        "!git clone https://github.com/BandaiNamcoResearchInc/Bandai-Namco-Research-Motiondataset.git\n",
        "\n",
        "# copy Bandai data to root for ease of access\n",
        "!mkdir bandai_samples\n",
        "!cp -r Bandai-Namco-Research-Motiondataset/dataset/Bandai-Namco-Research-Motiondataset-1/data/* bandai_samples"
      ],
      "metadata": {
        "id": "sXmLJ_-WFt-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare numpy arrays using joint world positions of the Bandai samples\n",
        "import os\n",
        "import pickle\n",
        "import bvhio\n",
        "import numpy as np\n",
        "from natsort import natsorted\n",
        "\n",
        "animation_names =[ # bow\n",
        "    \"bow_active\", # 0\n",
        "    \"bow_angry\", # 1\n",
        "    \"bow_childish\", # 2\n",
        "    \"bow_chimpira\", # 3\n",
        "    \"bow_feminine\", # 4\n",
        "    \"bow_giant\", # 5\n",
        "    \"bow_happy\", # 6\n",
        "    \"bow_masculinity\", # 7\n",
        "    \"bow_musical\", # 8\n",
        "    \"bow_normal\", # 9\n",
        "    \"bow_not-confident\", # 10\n",
        "    \"bow_old\", # 11\n",
        "    \"bow_proud\", # 12\n",
        "    \"bow_sad\", # 13\n",
        "    \"bow_tired\", # 14\n",
        "    # bye\n",
        "    \"bye_active\", # 0\n",
        "    \"bye_angry\", # 1\n",
        "    \"bye_childish\", # 2\n",
        "    \"bye_chimpira\", # 3\n",
        "    \"bye_feminine\", # 4\n",
        "    \"bye_giant\", # 5\n",
        "    \"bye_happy\", # 6\n",
        "    \"bye_masculinity\", # 7\n",
        "    \"bye_musical\", # 8\n",
        "    \"bye_normal\", # 9\n",
        "    \"bye_not-confident\", # 10\n",
        "    \"bye_old\", # 11\n",
        "    \"bye_proud\", # 12\n",
        "    \"bye_sad\", # 13\n",
        "    \"bye_tired\", # 14\n",
        "    # byebye\n",
        "    \"byebye_active\", # 0\n",
        "    \"byebye_angry\", # 1\n",
        "    \"byebye_childish\", # 2\n",
        "    \"byebye_chimpira\", # 3\n",
        "    \"byebye_feminine\", # 4\n",
        "    \"byebye_giant\", # 5\n",
        "    \"byebye_happy\", # 6\n",
        "    \"byebye_masculinity\", # 7\n",
        "    \"byebye_musical\", # 8\n",
        "    \"byebye_normal\", # 9\n",
        "    \"byebye_not-confident\", # 10\n",
        "    \"byebye_old\", # 11\n",
        "    \"byebye_proud\", # 12\n",
        "    \"byebye_sad\", # 13\n",
        "    \"byebye_tired\", # 14\n",
        "    # dash\n",
        "    \"dash_active\", # 0\n",
        "    \"dash_angry\", # 1\n",
        "    \"dash_childish\", # 2\n",
        "    \"dash_chimpira\", # 3\n",
        "    \"dash_feminine\", # 4\n",
        "    \"dash_giant\", # 5\n",
        "    \"dash_happy\", # 6\n",
        "    \"dash_masculinity\", # 7\n",
        "    \"dash_musical\", # 8\n",
        "    \"dash_normal\", # 9\n",
        "    \"dash_not-confident\", # 10\n",
        "    \"dash_old\", # 11\n",
        "    \"dash_proud\", # 12\n",
        "    \"dash_sad\", # 13\n",
        "    \"dash_tired\", # 14\n",
        "    # guide\n",
        "    \"guide_active\", # 0\n",
        "    \"guide_angry\", # 1\n",
        "    \"guide_childish\", # 2\n",
        "    \"guide_chimpira\", # 3\n",
        "    \"guide_feminine\", # 4\n",
        "    \"guide_giant\", # 5\n",
        "    \"guide_happy\", # 6\n",
        "    \"guide_masculinity\", # 7\n",
        "    \"guide_musical\", # 8\n",
        "    \"guide_normal\", # 9\n",
        "    \"guide_not-confident\", # 10\n",
        "    \"guide_old\", # 11\n",
        "    \"guide_proud\", # 12\n",
        "    \"guide_sad\", # 13\n",
        "    \"guide_tired\", # 14\n",
        "    # run\n",
        "    \"run_active\", # 0\n",
        "    \"run_angry\", # 1\n",
        "    \"run_childish\", # 2\n",
        "    \"run_chimpira\", # 3\n",
        "    \"run_feminine\", # 4\n",
        "    \"run_giant\", # 5\n",
        "    \"run_happy\", # 6\n",
        "    \"run_masculinity\", # 7\n",
        "    \"run_musical\", # 8\n",
        "    \"run_normal\", # 9\n",
        "    \"run_not-confident\", # 10\n",
        "    \"run_old\", # 11\n",
        "    \"run_proud\", # 12\n",
        "    \"run_sad\", # 13\n",
        "    \"run_tired\", # 14\n",
        "     # walk\n",
        "    \"walk_active_1\", # 0\n",
        "    \"walk_angry_1\", # 1\n",
        "    \"walk_childish_1\", # 2\n",
        "    \"walk_chimpira_1\", # 3\n",
        "    \"walk_feminine_1\", # 4\n",
        "    \"walk_giant_1\", # 5\n",
        "    \"walk_happy_1\", # 6\n",
        "    \"walk_masculinity_1\", # 7\n",
        "    \"walk_musical_1\", # 8\n",
        "    \"walk_normal_1\", # 9\n",
        "    \"walk_not-confident_1\", # 10\n",
        "    \"walk_old_1\", # 11\n",
        "    \"walk_proud_1\", # 12\n",
        "    \"walk_sad_1\", # 13\n",
        "    \"walk_tired_1\", # 14\n",
        "    \"walk_active_2\", # 0\n",
        "    \"walk_angry_2\", # 1\n",
        "    \"walk_childish_2\", # 2\n",
        "    \"walk_chimpira_2\", # 3\n",
        "    \"walk_feminine_2\", # 4\n",
        "    \"walk_giant_2\", # 5\n",
        "    \"walk_happy_2\", # 6\n",
        "    \"walk_masculinity_2\", # 7\n",
        "    \"walk_musical_2\", # 8\n",
        "    \"walk_normal_2\", # 9\n",
        "    \"walk_not-confident_2\", # 10\n",
        "    \"walk_old_2\", # 11\n",
        "    \"walk_proud_2\", # 12\n",
        "    \"walk_sad_2\", # 13\n",
        "    \"walk_tired_2\", # 14\n",
        "    # other\n",
        "    \"call_normal_1\",\n",
        "    \"call_normal_2\",\n",
        "    \"kick_normal\",\n",
        "    \"punch_normal_1\",\n",
        "    \"punch_normal_2\",\n",
        "    \"respond_normal\",\n",
        "    \"slash_normal_1\",\n",
        "    \"slash_normal_2\"\n",
        "]\n",
        "\n",
        "directory_bandai = \"bandai_samples/\"\n",
        "\n",
        "def toFileName(aniName):\n",
        "    if(aniName[-2:] == '_1'):\n",
        "        return directory_bandai + 'dataset-1_' + aniName[:-2] + '_001.bvh'\n",
        "    elif(aniName[-2:] == '_2'):\n",
        "        return directory_bandai + 'dataset-1_' + aniName[:-2] + '_002.bvh'\n",
        "    else:\n",
        "        return directory_bandai + 'dataset-1_' + aniName + '_001.bvh'\n",
        "\n",
        "bodyParts_bandai = ['Hips', 'Spine', 'Chest', 'Neck', 'Head', 'Shoulder_L', 'UpperArm_L', 'LowerArm_L', 'Hand_L',\n",
        "            'Shoulder_R', 'UpperArm_R', 'LowerArm_R', 'Hand_R', 'UpperLeg_L', 'LowerLeg_L', 'Foot_L',\n",
        "            'Toes_L', 'UpperLeg_R', 'LowerLeg_R', 'Foot_R', 'Toes_R']\n",
        "\n",
        "joint_per_pose = len(bodyParts_bandai)\n",
        "\n",
        "bvhanis_bandai = []\n",
        "\n",
        "for aniName in animation_names:\n",
        "    bvhanis_bandai.append(bvhio.readAsHierarchy(toFileName(aniName)))\n",
        "\n",
        "jointsall_bandai = []\n",
        "\n",
        "for ind, root in enumerate(bvhanis_bandai):\n",
        "    frame_count = len(root.Keyframes)\n",
        "    joints = np.empty((frame_count,joint_per_pose),dtype=object)\n",
        "\n",
        "    for frame in range(frame_count):\n",
        "        loadedPose = root.loadPose(frame)\n",
        "\n",
        "        for partin,part in enumerate(bodyParts_bandai):\n",
        "            joints[frame][partin] = loadedPose.filter(part)[0].PositionWorld\n",
        "\n",
        "    jointsall_bandai.append(joints)\n",
        "\n",
        "file = open('bandai_joints.pickle', 'wb')\n",
        "pickle.dump(jointsall_bandai, file)\n",
        "file.close()"
      ],
      "metadata": {
        "id": "LTR9xuFuFZeX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}